{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project Starter Code\n",
    "This guide will help you to take a first step into deep learning, you will learn how manipulate your data and train your neural network. By the end of this guide, you should have your neural network model trained and saved on disk, and then you can use that model on real-time detection.\n",
    "\n",
    "Most of the code in this guide has been implemented for you, and you have very little work to do here, because the goal is to help you get some quick experence on training your first neural network. If you have sufficient knowledge feel free to modify anything to achieve better performance. If you are a beginner, please carefully read the code in this notebook and try to understand, and follow all the steps.\n",
    "\n",
    "Before starting, please make sure the following Python package are installed:\n",
    "- NumPy\n",
    "- OpenCV\n",
    "- Tensorflow\n",
    "- Matplotlib\n",
    "- SciPy\n",
    "- Pillow\n",
    "\n",
    "Run the following code cell by clicking the \"Run\" button, make sure all modules can be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize\n",
    "from utils import load_ubyte\n",
    "from model import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data\n",
    "\n",
    "Please go to [MNIST official website](http://yann.lecun.com/exdb/mnist/) and download the following files:\n",
    "\n",
    "- train-images-idx3-ubyte.gz\n",
    "- train-labels-idx1-ubyte.gz\n",
    "- t10k-images-idx3-ubyte.gz\n",
    "- t10k-labels-idx1-ubyte.gz\n",
    "\n",
    "These are the trainning data and testing data you will be using to train your neural\n",
    "network on. Please put them in the same directory as this tutorial.\n",
    "\n",
    "The data files has extension name of \"gz\", because they are in gzip compressed format, we need to decompress them first. The command for decompressing the first file is:\n",
    "```bash\n",
    "gzip -d train-images-idx3-ubyte.gz\n",
    "```\n",
    "Please decompress all data using this command, if you see an error, make sure gzip is correctly installed on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MNIST data\n",
    "Once you decompressed all the above files, you'll find that the file extension \"gz\" are removed, now all data files are in the raw ubyte format. We need to load all data into NumPy array so that we can apply algorithms in NumPy, OpenCV, etc. To help you start, there is already a function called load_ubyte in utils.py that does the job for you.\n",
    "\n",
    "We need to call load_ubyte to load our data, the first parameter should be the image file, and the second parameter should be the label file. Please run the cell below. This function might takes a long time to run depending on your machine, please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing data:\n",
    "test_x, test_y = load_ubyte('t10k-images-idx3-ubyte', 't10k-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to take a deeper look at our data to make sure everything is correct. We want to check the structure, type and scale of the data. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data we just loaded:\n",
    "print test_x.shape\n",
    "print test_y.shape\n",
    "print test_x.dtype\n",
    "print test_y.dtype\n",
    "print test_x.max(), test_x.min()\n",
    "print test_y.max(), test_y.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are what we have: our testing data is stored in two variables, test_x stores the input data and test_y stores the labels. The input data are 10000 images of 28x28 in resolution, and the labels are 10000 numbers correspond to each image. The input data are uint8 because the pixels of image are usually 0~255, this is the right format for displaying image, but if we want to perform numeric computation we might need to first convert uint8 into float32 or float64, otherwise we could lose percision. The labels are already in float64, which is okay to do computation directly, you might optionally want to convert that into float32 to save some space, but that usually doesn't matter. Note that the label ranging from 0 to 9, which corresponding to the number in each image, but some package might assume the label start from 1 rather than 0, in this case you simply add one to the label. Always check your dataset to prevent problem like this.\n",
    "\n",
    "The next thing we want to do is randomly show an image and corresponding label to see if our data is correctly labeled, you can run the cell below multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, test_x.shape[0]) # choose a number from 0 to 9999\n",
    "plt.imshow(test_x[idx, :, :, 0], cmap='gray') # Display the image, use grayscale colormap because our image is grayscale\n",
    "plt.show() # plt requires calling this function to make image actually show up\n",
    "\n",
    "print 'This number is: ', test_y[idx, 0] # Print the corresponding label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is your turn, load the training data (which contains 60000 images and labels), check the data shape, type, cales, etc. Play around with the data. Make sure that your training data is stored as variables train_x and train_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation\n",
    "As mentioned in the presentation, this dataset is too \"perfect\" in the sense that it doesn't looks like what we will see in real camera frame. Specifically, in reality, the image you get from camera should contains background noises, different light conditions, overexposures, or the target doesn't aligned in the center, etc. So if you train the neural network directly on this dataset you cannot get a very rubost model.\n",
    "\n",
    "That is why we need to do some manipulation on our dataset before feed them into the neural networks. Here are the operations we need to perform on the dataset: random shifting, random rotation, adding background noise, and more.\n",
    "\n",
    "### 3.1 Random Shifting\n",
    "We need to define a function that can perform a same operation on multiple images, so that later we can do data augmentation more convenient.\n",
    "\n",
    "The idea here is to shift the digit by a small random offset. Please go to this [tutorial](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html) to learn the basics of geometry operations, specifically how to use *warpAffine* function to do translation (shifting). After that, implement random_shift function below, check the your result and make sure that the number don't shift too far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shift(input_image):\n",
    "    # TODO\n",
    "    return input_image\n",
    "\n",
    "input_image = test_x[random.randint(0, 10000), :, :, 0] # select a random image\n",
    "output_image = random_shift(input_image)\n",
    "plt.subplot(211)\n",
    "plt.imshow(input_image, cmap='gray')\n",
    "plt.subplot(212)\n",
    "plt.imshow(output_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Rotation\n",
    "Implement random_rotate function, this time rotate the input image by a small random angle. Again study *warpAffine* function to learn how to do rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotate(input_image):\n",
    "    # TODO\n",
    "    return input_image\n",
    "\n",
    "input_image = test_x[random.randint(0, 10000), :, :, 0] # select a random image\n",
    "output_image = random_shift(input_image)\n",
    "plt.subplot(211)\n",
    "plt.imshow(input_image, cmap='gray')\n",
    "plt.subplot(212)\n",
    "plt.imshow(output_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Add Random Noise\n",
    "The idea here is that we generate a matrix of random numbers, and  add add this matrix to the input image. The key problem here is that you want to make sure that the value of the resuling image doesn't fall outsite of 0 to 255, otherwise the image won't display correctly. Read the following code carefully. Run the code and see the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(input_image):\n",
    "    # Get image height and width:\n",
    "    height, width = input_image.shape\n",
    "    \n",
    "    # Convert the image to float32 to allow arithmetic operations:\n",
    "    input_image = input_image.astype(np.float32)\n",
    "    \n",
    "    # Generate a matrix of random number, the random numbers were drawn from normal distribution with mean and std:\n",
    "    mean = 0.0\n",
    "    std = 50.0\n",
    "    noise = np.random.normal(mean, std, [height, width])\n",
    "    \n",
    "    # Add noise to the image:\n",
    "    input_image += noise\n",
    "    \n",
    "    # Cut off out-of-range values by setting them equal to the bound:\n",
    "    input_image[input_image < 0] = 0.0\n",
    "    input_image[input_image > 255] = 255.0\n",
    "    \n",
    "    # Convert back to uint8 to allow displaying:\n",
    "    input_image = input_image.astype(np.uint8)\n",
    "    return input_image\n",
    "\n",
    "input_image = test_x[random.randint(0, 10000), :, :, 0]\n",
    "output_image = add_noise(input_image)\n",
    "plt.subplot(211)\n",
    "plt.imshow(input_image, cmap='gray')\n",
    "plt.subplot(212)\n",
    "plt.imshow(output_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is to invert the image, because our input image should be black on white, this can be done simply by 255 minus all values in the image, thus it is important to make sure all values of image are within 0 to 255. Below is a small test that takes a random image and pass through all the transform functions we implemented, and invert the color in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_image(input_image):\n",
    "    return 255 - input_image\n",
    "\n",
    "input_image = test_x[random.randint(0, 10000), :, :, 0]\n",
    "output_image = random_shift(input_image)\n",
    "output_image = random_rotate(output_image)\n",
    "output_image = add_noise(output_image)\n",
    "output_image = invert_image(output_image)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(input_image, cmap='gray')\n",
    "plt.subplot(212)\n",
    "plt.imshow(output_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do data augmentation, the following script will take all training data and testing data, apply all the random transformations above, and generate an output dataset with transformed data. You can specify how many data you want, because the data is generated randomly, theoretically you can get infinitly amount of data, that is why this procedure is called data augmentation. But for this small task about 8000 training data and 2000 testing data should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(input_images, input_labels, output_size):\n",
    "    num, height, width, ch = input_images.shape\n",
    "    output_images = np.zeros([output_size, height, width, 1], dtype=np.uint8)\n",
    "    output_labels = np.zeros([output_size, 1], dtype=np.float32)\n",
    "    for i in xrange(output_size):\n",
    "        idx = random.randint(0, num)\n",
    "        img = input_images[idx, :, :, 0]\n",
    "        img = random_shift(img)\n",
    "        img = random_rotate(img)\n",
    "        img = add_noise(img)\n",
    "        img = invert_image(img)\n",
    "        output_images[i, :, :, 0] = img\n",
    "        output_labels[i, 0] = input_labels[idx, 0]\n",
    "    return output_images, output_labels\n",
    "\n",
    "training_size = 8000\n",
    "aug_train_x, aug_train_y = data_augmentation(train_x, train_y, training_size)\n",
    "\n",
    "testing_size = 2000\n",
    "aug_test_x, aug_test_y = data_augmentation(test_x, test_y, testing_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to normalize the images (i.e. x) so that their values are within -1.0 to 1.0. This is especially important for features that have different scales, for example, some dataset may have x0 between 0.0 to 1.0 and x1 between -999999.0 to 999999.0, these kind of values will make the neural netwok very unstable, and very hard to train. Although for our dataset, all pixel values are ranging from 0 to 255, which have the same scale, it is usually good practice to re-scale their values before feeding into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # First convert x into float:\n",
    "    ret = x.astype(np.float32)\n",
    "    \n",
    "    # We know the min value is 0 and the max value for x is 255,\n",
    "    # so the calculation is straightforward:\n",
    "    return ret / 255.0 * 2.0 - 1.0\n",
    "\n",
    "aug_train_x = normalize(aug_train_x)\n",
    "aug_test_x = normalize(aug_test_x)\n",
    "\n",
    "# Check the max and min values:\n",
    "print aug_train_x.max(), aug_train_x.min()\n",
    "print aug_test_x.max(), aug_test_x.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last thing before training, we need to convert the label (i.e. y) into \"one hot\" vector form. For example, if we have a vector \\[0, 4, 1, 5\\], then the corresponding \"one hot\" vector form is\n",
    "\n",
    "\\[\n",
    "\n",
    "  \\[1, 0, 0, 0, 0, 0, 0, 0, 0, 0\\],\n",
    "\n",
    "  \\[0, 0, 0, 0, 1, 0, 0, 0, 0, 0\\],\n",
    "  \n",
    "  \\[0, 1, 0, 0, 0, 0, 0, 0, 0, 0\\],\n",
    "  \n",
    "  \\[0, 0, 0, 0, 0, 1, 0, 0, 0, 0\\]\n",
    "  \n",
    "\\]\n",
    "\n",
    "This format is required for multi-class classifier in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(y, num_class):\n",
    "    num = y.shape[0]\n",
    "    ret = np.zeros([num, num_class])\n",
    "    for i in xrange(num):\n",
    "        ret[i, int(y[i, 0])] = 1\n",
    "    return ret\n",
    "\n",
    "one_hot_train_y = convert_to_one_hot(aug_train_y, 10)\n",
    "one_hot_test_y = convert_to_one_hot(aug_test_y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "Finally, we can start training our neural network on augmented dataset. Before starting, check the shape, scales, and type of aug_train_x and aug_test_x. You can also plot some random image to see if the format is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print aug_train_x.shape\n",
    "print aug_test_x.shape\n",
    "print one_hot_train_y.shape\n",
    "print one_hot_test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a 2 layer convolutional neural network(CNN), if you want to know more about CNN there are lots of online tutorials like [cs231n](http://cs231n.stanford.edu/syllabus.html) or [udacity deep learning](https://www.udacity.com/course/deep-learning--ud730). The model is already implemented for you, so you can start training right away, but feel free to modify the neural network archietecture in ```./model.py``` and ```./layers.py```. You should see the loss decreases and the training accuracy increases over time. In the end, you should see the test accuracy slightly less than the training accuracy, but it should be greater than 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training Parameters:\n",
    "learning_rate = 1e-3\n",
    "max_iter = 2000\n",
    "print_every = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Optimizer (the objective for training is to minimize loss, which is defined in model.py):\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Launch Session:\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# This is used for saving the trained model:\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initialize all model variables:\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# We are using mini-batch gradient descent, i.e. train the model on a\n",
    "# small subset(a batch) for each step, because this can save lots of RAM\n",
    "# while maintaining an acceptable accuracy.\n",
    "def next_batch(x, y, batch_size):\n",
    "    num_samples = x.shape[0]\n",
    "    if batch_size > num_samples:\n",
    "        raise ValueError('batch size cannot larger than data size')\n",
    "    else:\n",
    "        idx = range(num_samples)\n",
    "        random.shuffle(idx)\n",
    "        idx = idx[:batch_size]\n",
    "        return x[idx, ...], y[idx, ...]\n",
    "\n",
    "# Training:\n",
    "for i in range(max_iter):\n",
    "    batch = next_batch(aug_train_x, one_hot_train_y, batch_size)\n",
    "    #print batch[0].shape, batch[1].shape\n",
    "    if i%print_every == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "        #print out_fc.eval(feed_dict={x: batch[0], y_:batch[1]})\n",
    "        train_loss = loss.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "        #output = out_readout.eval(feed_dict={x: batch[0]})\n",
    "        print 'step %d, loss %g, training accuracy: %g' %(i, train_loss, train_accuracy)\n",
    "    train_step.run(feed_dict={\n",
    "        x: batch[0], y_: batch[1]\n",
    "        })\n",
    "    \n",
    "# Test the model on test set:\n",
    "test_accuracy = accuracy.eval(feed_dict={x: aug_test_x, y_: one_hot_test_y})\n",
    "print 'Test accuracy %g' %(test_accuracy)\n",
    "    \n",
    "# Save the trained model for real-time detector:\n",
    "save_path = saver.save(sess, 'model/model.ckpt')\n",
    "print 'model has been saved to', save_path\n",
    "\n",
    "# Close the session:\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-time Detection\n",
    "After training, please close this notebook and run ```./real_time_detection.py```, this program will load the model you trained to do real-time number recognition. You should draw a number on paper or anything, and show that number to the camera, the prediction will be printed on the center of the screen. Please try multiple times, if you don't satisfy the result, please back here to modify the training parameters and/or data augmentation process and train the model again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
